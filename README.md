# Sistema Distribuido de Log con OrdenaciÃ³n Causal

Sistema distribuido que implementa los algoritmos de **Reloj LÃ³gico de Lamport** y **Algoritmo Bully** para elecciÃ³n de lÃ­der, con replicaciÃ³n de mensajes.

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un sistema de log distribuido que garantiza:
- **OrdenaciÃ³n causal de mensajes** mediante Reloj LÃ³gico de Lamport
- **ElecciÃ³n automÃ¡tica de lÃ­der** mediante Algoritmo Bully
- **ReplicaciÃ³n de mensajes** entre todos los nodos del cluster
- **Tolerancia a fallos** con re-elecciÃ³n automÃ¡tica de lÃ­der

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Node 1    â”‚â”€â”€â”€â”€â–¶â”‚   Node 2    â”‚â”€â”€â”€â”€â–¶â”‚   Node 3    â”‚
â”‚  (Follower) â”‚     â”‚  (LEADER)   â”‚     â”‚  (Follower) â”‚
â”‚  Port 8001  â”‚â—€â”€â”€â”€â”€â”‚  Port 8002  â”‚â—€â”€â”€â”€â”€â”‚  Port 8003  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚                   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              ReplicaciÃ³n de mensajes
```

### Algoritmos Implementados

#### 1. Reloj LÃ³gico de Lamport
- Cada mensaje recibe un timestamp lÃ³gico Ãºnico
- Garantiza ordenaciÃ³n causal: si A â†’ B, entonces Lamport(A) < Lamport(B)
- SincronizaciÃ³n mediante regla: `time = max(local_time, remote_time) + 1`

#### 2. Algoritmo Bully (ElecciÃ³n de LÃ­der)
- El nodo con mayor ID es siempre el lÃ­der
- DetecciÃ³n automÃ¡tica de fallos (health check cada 5 segundos)
- Re-elecciÃ³n automÃ¡tica cuando el lÃ­der cae

#### 3. ReplicaciÃ³n Single-Leader
- Solo el lÃ­der acepta escrituras
- Followers redirigen mensajes al lÃ­der
- LÃ­der replica a todos los followers

## ğŸš€ Inicio RÃ¡pido

### Requisitos
- Docker
- Docker Compose (opcional)
- Bash

### OpciÃ³n 1: Cluster Local (Desarrollo)

```bash
# 1. Dar permisos de ejecuciÃ³n a los scripts
chmod +x test-local.sh stop-local.sh test-send-messages.sh

# 2. Iniciar cluster de 3 nodos
./test-local.sh

# 3. Esperar 10 segundos para que se elija lÃ­der

# 4. Enviar mensajes de prueba
./test-send-messages.sh

# 5. Ver logs de un nodo
docker logs -f node1

# 6. Detener cluster
./stop-local.sh
```

### OpciÃ³n 2: Nodo Individual

```bash
# Build de la imagen
docker build -t distributed-log:latest .

# Ejecutar un nodo
docker run -d \
  --name node1 \
  -p 8001:80 \
  -e NODE_ID=8001 \
  distributed-log:latest
```

## ğŸ“¡ API Endpoints

### Endpoints PÃºblicos

#### `GET /`
Obtener un mensaje especÃ­fico por ID.
```bash
curl "http://localhost:8001/?id=1"
```

#### `POST /`
Crear un nuevo mensaje (serÃ¡ replicado a todos los nodos).
```bash
curl -X POST "http://localhost:8001/?message=Hello World"
```
**Respuesta:**
```json
{
  "id": 2,
  "lamport_timestamp": 15,
  "node_id": 8002
}
```

#### `GET /state`
Ver el estado del nodo (ID, lÃ­der actual).
```bash
curl http://localhost:8001/state
```

#### `GET /dashboard`
**ğŸ¨ Dashboard Web Interactivo** - La mejor forma de visualizar el sistema.
```bash
# En el navegador, abre:
http://localhost:8001/dashboard
http://localhost:8002/dashboard
http://localhost:8003/dashboard
```

**CaracterÃ­sticas:**
- âœ… **VisualizaciÃ³n en tiempo real** del estado de todos los nodos
- âœ… **Enviar mensajes** directamente desde la interfaz
- âœ… **Ver mensajes ordenados** por Lamport timestamp
- âœ… **Auto-refresh** cada 3 segundos
- âœ… **Indicador visual** del nodo lÃ­der (ğŸ‘‘)
- âœ… **DiseÃ±o responsive** perfecto para demos y videos ğŸ“¹

#### `GET /leader`
Obtener el ID del lÃ­der actual.
```bash
curl http://localhost:8001/leader
```

#### `GET /health`
Health check del nodo.
```bash
curl http://localhost:8001/health
```

#### `GET /lamport_time`
Obtener el Lamport timestamp actual del nodo.
```bash
curl http://localhost:8001/lamport_time
# Respuesta: {"time":5,"node_id":8001}
```

#### `GET /messages`
Obtener todos los mensajes ordenados por Lamport timestamp.
```bash
curl http://localhost:8001/messages
```

### Endpoints Internos (ReplicaciÃ³n)

#### `POST /message_received`
Recibir mensaje replicado desde el lÃ­der (uso interno).

#### `GET /leader_selected`
NotificaciÃ³n de nuevo lÃ­der (uso interno para Bully).

## ğŸ§ª Testing

### Test 1: EnvÃ­o de Mensajes
```bash
# Enviar mensaje a cualquier nodo (serÃ¡ redirigido al lÃ­der)
curl -X POST "http://localhost:8001/?message=Test 1"
curl -X POST "http://localhost:8002/?message=Test 2"
curl -X POST "http://localhost:8003/?message=Test 3"

# Verificar que todos tienen los mismos mensajes
curl http://localhost:8001/state
curl http://localhost:8002/state
curl http://localhost:8003/state
```

### Test 2: Tolerancia a Fallos (Algoritmo Bully)
```bash
# 1. Verificar quiÃ©n es el lÃ­der
LEADER=$(curl -s http://localhost:8001/leader)
echo "LÃ­der actual: $LEADER"

# 2. Matar al lÃ­der (deberÃ­a ser node3 con ID 8003)
docker stop node3

# 3. Esperar 10 segundos (detecciÃ³n + elecciÃ³n)
sleep 10

# 4. Verificar nuevo lÃ­der (deberÃ­a ser node2 con ID 8002)
NEW_LEADER=$(curl -s http://localhost:8001/leader)
echo "Nuevo lÃ­der: $NEW_LEADER"

# 5. Enviar mensaje con el nuevo lÃ­der
curl -X POST "http://localhost:8001/?message=After failover"

# 6. Restaurar el nodo original
docker start node3

# 7. Esperar 10 segundos (node3 se vuelve lÃ­der por tener mayor ID)
sleep 10

# 8. Verificar lÃ­der final
FINAL_LEADER=$(curl -s http://localhost:8001/leader)
echo "LÃ­der final: $FINAL_LEADER"  # DeberÃ­a ser 8003 nuevamente
```

### Test 3: OrdenaciÃ³n Causal (Lamport)
```bash
# Enviar mensajes concurrentes desde diferentes nodos
for i in {1..10}; do
  curl -X POST "http://localhost:8001/?message=Concurrent_$i" &
  curl -X POST "http://localhost:8002/?message=Concurrent_$i" &
  curl -X POST "http://localhost:8003/?message=Concurrent_$i" &
done
wait

# Los mensajes deberÃ­an estar ordenados por timestamp Lamport
# (verificar en los logs de cada nodo)
```

## ğŸ³ Deployment en GCP

### Prerequisitos
- Cuenta de Google Cloud Platform
- `gcloud` CLI instalado y autenticado (`gcloud auth login`)
- Proyecto GCP creado con billing habilitado
- Docker instalado localmente

### OpciÃ³n 1: Deployment AutomÃ¡tico (RECOMENDADO)

**Script completo que hace todo:**

```bash
# 1. Configurar proyecto
export GCP_PROJECT_ID="tu-proyecto-id"

# 2. Ejecutar deployment automÃ¡tico
./deploy-gcp.sh
```

Este script automÃ¡ticamente:
- âœ… Habilita APIs necesarias (Compute Engine, Artifact Registry, Container Registry)
- âœ… Crea firewall rules
- âœ… Build y push de imagen Docker a GCR
- âœ… Crea 3 VMs en regiones distantes (Iowa, SÃ£o Paulo, Sydney)
- âœ… Instala Docker en cada VM
- âœ… Inicia contenedores con configuraciÃ³n correcta

**IMPORTANTE:** El primer deployment puede fallar en la comunicaciÃ³n entre nodos. Si esto ocurre, ejecuta:

```bash
# Re-deploy de contenedores con IPs correctas
./redeploy-containers.sh
```

Este script:
- Rebuild la imagen con el fix mÃ¡s reciente
- Obtiene las IPs pÃºblicas de las VMs
- Recrea los contenedores pasando `OTHER_SERVERS` con las IPs correctas

### OpciÃ³n 2: Deployment Manual Paso a Paso

```bash
# 1. Configurar proyecto
export GCP_PROJECT_ID="tu-proyecto-id"
gcloud config set project $GCP_PROJECT_ID

# 2. Habilitar APIs necesarias
gcloud services enable compute.googleapis.com
gcloud services enable artifactregistry.googleapis.com
gcloud services enable containerregistry.googleapis.com

# 3. Crear firewall rules
gcloud compute firewall-rules create allow-distributed-log \
  --allow=tcp:80,tcp:443,tcp:8000-8100 \
  --target-tags=distributed-log

# 4. Build y push de imagen a GCR
docker build -t gcr.io/$GCP_PROJECT_ID/distributed-log:latest .
gcloud auth configure-docker
docker push gcr.io/$GCP_PROJECT_ID/distributed-log:latest

# 5. Crear VMs (esto toma 2-3 minutos)
# Ver secciÃ³n "CreaciÃ³n Manual de VMs" mÃ¡s abajo

# 6. Una vez creadas las VMs, ejecutar redeploy
./redeploy-containers.sh
```

### Verificar Deployment

```bash
# Ver estado de los nodos
./check-gcp-status.sh
```

DeberÃ­as ver algo como:
```
log-node-1 (NODE_ID=8001)
âœ“ HTTP responding
  Lamport Time: {"time":0,"node_id":8001}
  Leader ID: 8003  â† Todos deberÃ­an tener el mismo lÃ­der

log-node-2 (NODE_ID=8002)
âœ“ HTTP responding
  Leader ID: 8003

log-node-3 (NODE_ID=8003)
âœ“ HTTP responding
  Leader ID: 8003  â† Este es el lÃ­der (mayor ID)
```

### Scripts Disponibles para GCP

| Script | DescripciÃ³n |
|--------|-------------|
| `./deploy-gcp.sh` | Deployment inicial completo (crea VMs, firewall, etc.) |
| `./redeploy-containers.sh` | Re-deployar solo los contenedores con nueva configuraciÃ³n |
| `./check-gcp-status.sh` | Verificar estado de todos los nodos |
| `./debug-node.sh <num>` | Ver logs detallados de un nodo especÃ­fico (ej: `./debug-node.sh 3`) |
| `./destroy-gcp.sh` | Eliminar toda la infraestructura de GCP |

### Testing en GCP

```bash
# Obtener IPs de las VMs (ejecutar check-gcp-status.sh primero)
# O manualmente:
IP1="34.55.87.209"    # log-node-1 (Iowa)
IP2="34.95.212.100"   # log-node-2 (SÃ£o Paulo)
IP3="35.201.29.184"   # log-node-3 (Sydney)

# Enviar mensaje al lÃ­der (node 3)
curl -X POST "http://$IP3/?message=Hello_from_Sydney"

# Verificar que se replicÃ³ en todos los nodos
curl http://$IP1/messages
curl http://$IP2/messages
curl http://$IP3/messages

# Todos deberÃ­an mostrar el mismo mensaje con el mismo Lamport timestamp
```

### Deploy Manual en VM (Avanzado)

Si necesitas deployar manualmente en una VM especÃ­fica:

```bash
# SSH a la VM
gcloud compute ssh log-node-1 --zone=us-central1-a

# Obtener IPs de TODAS las VMs primero
# IP1=... IP2=... IP3=...

# Parar contenedor viejo
docker stop distributed-log 2>/dev/null || true
docker rm distributed-log 2>/dev/null || true

# Pull imagen
gcloud auth configure-docker
docker pull gcr.io/$GCP_PROJECT_ID/distributed-log:latest

# Ejecutar contenedor con OTHER_SERVERS
docker run -d \
  --name distributed-log \
  --restart unless-stopped \
  -p 80:80 \
  -e NODE_ID=8001 \
  -e OTHER_SERVERS="$IP1:80:8001,$IP2:80:8002,$IP3:80:8003" \
  gcr.io/$GCP_PROJECT_ID/distributed-log:latest

# Verificar
docker ps
docker logs distributed-log --tail 50
```

## ğŸ“Š Estructura del Proyecto

```
.
â”œâ”€â”€ main.py                    # AplicaciÃ³n principal FastAPI
â”œâ”€â”€ server.py                  # Modelo de servidor
â”œâ”€â”€ lamport_clock.py          # ImplementaciÃ³n Reloj LÃ³gico (TODO)
â”œâ”€â”€ metrics.py                # Colector de mÃ©tricas (TODO)
â”œâ”€â”€ dockerfile                # Imagen Docker
â”œâ”€â”€ requirements.txt          # Dependencias Python
â”œâ”€â”€ test-local.sh            # Script para testing local
â”œâ”€â”€ stop-local.sh            # Script para detener cluster
â”œâ”€â”€ test-send-messages.sh    # Script para enviar mensajes de prueba
â””â”€â”€ README.md                # Este archivo
```

## ğŸ”§ Variables de Entorno

| Variable  | DescripciÃ³n                    | Ejemplo | Requerido |
|-----------|--------------------------------|---------|-----------|
| NODE_ID   | ID Ãºnico del nodo              | 8001    | âœ… SÃ­     |
| OTHER_SERVERS | IPs de otros nodos (GCP)   | 34.55.87.209:80:8001,... | Solo en GCP |

**Formato de OTHER_SERVERS:**
```
ip1:puerto1:id1,ip2:puerto2:id2,ip3:puerto3:id3
```

**Ejemplo para GCP:**
```bash
OTHER_SERVERS="34.55.87.209:80:8001,34.95.212.100:80:8002,35.201.29.184:80:8003"
```

**Nota:** En Docker local, esta variable NO es necesaria. El sistema usa automÃ¡ticamente los nombres de contenedores (node1, node2, node3).

## ğŸ“‹ Tabla Original de Variables

| Variable  | DescripciÃ³n                    | Ejemplo |
|-----------|--------------------------------|---------|
| NODE_ID   | ID Ãºnico del nodo (requerido)  | 8001    |

## ğŸ“ Logs

Los logs se muestran en stdout de cada contenedor:

```bash
# Ver logs en tiempo real
docker logs -f node1

# Ver Ãºltimas 100 lÃ­neas
docker logs --tail 100 node1

# Logs de todos los nodos
docker logs node1 > logs/node1.log
docker logs node2 > logs/node2.log
docker logs node3 > logs/node3.log
```

## ğŸ› Troubleshooting

### Los nodos no se comunican
```bash
# Verificar que estÃ¡n en la misma red Docker
docker network inspect distributed-net

# Verificar conectividad
docker exec node1 ping -c 3 node2
docker exec node1 ping -c 3 node3
```

### No se elige lÃ­der
```bash
# Verificar logs de elecciÃ³n
docker logs node1 | grep -i "election\|leader"

# Forzar re-elecciÃ³n matando el lÃ­der actual
docker stop node3
```

### Mensajes no se replican
```bash
# Verificar que el nodo es lÃ­der
curl http://localhost:8001/leader

# Verificar que followers estÃ¡n vivos
curl http://localhost:8002/health
curl http://localhost:8003/health
```

## ğŸ“š Referencias

- [Lamport Timestamps](https://lamport.azurewebsites.net/pubs/time-clocks.pdf) - Paper original de Leslie Lamport
- [Bully Algorithm](https://en.wikipedia.org/wiki/Bully_algorithm) - Wikipedia
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Docker Documentation](https://docs.docker.com/)

## ğŸ‘¥ Autores

- Sergio Sebastian Pezo Jimenez - RA: 298813
- Estudiante 2 - RA: XXXXXX

Projeto desenvolvido para a disciplina **MC714 - Sistemas DistribuÃ­dos**, Unicamp, 2Âº Semestre de 2025.

## ğŸ“„ Licencia

Este proyecto es para uso acadÃ©mico.
