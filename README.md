# Sistema Distribuido de Log con OrdenaciÃ³n Causal

Sistema distribuido que implementa los algoritmos de **Reloj LÃ³gico de Lamport** y **Algoritmo Bully** para elecciÃ³n de lÃ­der, con replicaciÃ³n de mensajes.

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un sistema de log distribuido que garantiza:
- **OrdenaciÃ³n causal de mensajes** mediante Reloj LÃ³gico de Lamport
- **ElecciÃ³n automÃ¡tica de lÃ­der** mediante Algoritmo Bully
- **ReplicaciÃ³n de mensajes** entre todos los nodos del cluster
- **Tolerancia a fallos** con re-elecciÃ³n automÃ¡tica de lÃ­der

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Node 1    â”‚â”€â”€â”€â”€â–¶â”‚   Node 2    â”‚â”€â”€â”€â”€â–¶â”‚   Node 3    â”‚
â”‚  (Follower) â”‚     â”‚  (LEADER)   â”‚     â”‚  (Follower) â”‚
â”‚  Port 8001  â”‚â—€â”€â”€â”€â”€â”‚  Port 8002  â”‚â—€â”€â”€â”€â”€â”‚  Port 8003  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚                   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              ReplicaciÃ³n de mensajes
```

### Algoritmos Implementados

#### 1. Reloj LÃ³gico de Lamport
- Cada mensaje recibe un timestamp lÃ³gico Ãºnico
- Garantiza ordenaciÃ³n causal: si A â†’ B, entonces Lamport(A) < Lamport(B)
- SincronizaciÃ³n mediante regla: `time = max(local_time, remote_time) + 1`

#### 2. Algoritmo Bully (ElecciÃ³n de LÃ­der)
- El nodo con mayor ID es siempre el lÃ­der
- DetecciÃ³n automÃ¡tica de fallos (health check cada 5 segundos)
- Re-elecciÃ³n automÃ¡tica cuando el lÃ­der cae

#### 3. ReplicaciÃ³n Single-Leader
- Solo el lÃ­der acepta escrituras
- Followers redirigen mensajes al lÃ­der
- LÃ­der replica a todos los followers

## ğŸš€ Inicio RÃ¡pido

### Requisitos
- Docker
- Docker Compose (opcional)
- Bash

### OpciÃ³n 1: Cluster Local (Desarrollo)

```bash
# 1. Dar permisos de ejecuciÃ³n a los scripts
chmod +x test-local.sh stop-local.sh test-send-messages.sh

# 2. Iniciar cluster de 3 nodos
./test-local.sh

# 3. Esperar 10 segundos para que se elija lÃ­der

# 4. Enviar mensajes de prueba
./test-send-messages.sh

# 5. Ver logs de un nodo
docker logs -f node1

# 6. Detener cluster
./stop-local.sh
```

### OpciÃ³n 2: Nodo Individual

```bash
# Build de la imagen
docker build -t distributed-log:latest .

# Ejecutar un nodo
docker run -d \
  --name node1 \
  -p 8001:80 \
  -e NODE_ID=8001 \
  distributed-log:latest
```

## ğŸ“¡ API Endpoints

### Endpoints PÃºblicos

#### `GET /`
Obtener un mensaje especÃ­fico por ID.
```bash
curl "http://localhost:8001/?id=1"
```

#### `POST /`
Crear un nuevo mensaje (serÃ¡ replicado a todos los nodos).
```bash
curl -X POST "http://localhost:8001/?message=Hello World"
```
**Respuesta:**
```json
{
  "id": 2,
  "lamport_timestamp": 15,
  "node_id": 8002
}
```

#### `GET /state`
Ver el estado del nodo (ID, lÃ­der actual).
```bash
curl http://localhost:8001/state
```

#### `GET /leader`
Obtener el ID del lÃ­der actual.
```bash
curl http://localhost:8001/leader
```

#### `GET /health`
Health check del nodo.
```bash
curl http://localhost:8001/health
```

### Endpoints Internos (ReplicaciÃ³n)

#### `POST /message_received`
Recibir mensaje replicado desde el lÃ­der (uso interno).

#### `GET /leader_selected`
NotificaciÃ³n de nuevo lÃ­der (uso interno para Bully).

## ğŸ§ª Testing

### Test 1: EnvÃ­o de Mensajes
```bash
# Enviar mensaje a cualquier nodo (serÃ¡ redirigido al lÃ­der)
curl -X POST "http://localhost:8001/?message=Test 1"
curl -X POST "http://localhost:8002/?message=Test 2"
curl -X POST "http://localhost:8003/?message=Test 3"

# Verificar que todos tienen los mismos mensajes
curl http://localhost:8001/state
curl http://localhost:8002/state
curl http://localhost:8003/state
```

### Test 2: Tolerancia a Fallos (Algoritmo Bully)
```bash
# 1. Verificar quiÃ©n es el lÃ­der
LEADER=$(curl -s http://localhost:8001/leader)
echo "LÃ­der actual: $LEADER"

# 2. Matar al lÃ­der (deberÃ­a ser node3 con ID 8003)
docker stop node3

# 3. Esperar 10 segundos (detecciÃ³n + elecciÃ³n)
sleep 10

# 4. Verificar nuevo lÃ­der (deberÃ­a ser node2 con ID 8002)
NEW_LEADER=$(curl -s http://localhost:8001/leader)
echo "Nuevo lÃ­der: $NEW_LEADER"

# 5. Enviar mensaje con el nuevo lÃ­der
curl -X POST "http://localhost:8001/?message=After failover"

# 6. Restaurar el nodo original
docker start node3

# 7. Esperar 10 segundos (node3 se vuelve lÃ­der por tener mayor ID)
sleep 10

# 8. Verificar lÃ­der final
FINAL_LEADER=$(curl -s http://localhost:8001/leader)
echo "LÃ­der final: $FINAL_LEADER"  # DeberÃ­a ser 8003 nuevamente
```

### Test 3: OrdenaciÃ³n Causal (Lamport)
```bash
# Enviar mensajes concurrentes desde diferentes nodos
for i in {1..10}; do
  curl -X POST "http://localhost:8001/?message=Concurrent_$i" &
  curl -X POST "http://localhost:8002/?message=Concurrent_$i" &
  curl -X POST "http://localhost:8003/?message=Concurrent_$i" &
done
wait

# Los mensajes deberÃ­an estar ordenados por timestamp Lamport
# (verificar en los logs de cada nodo)
```

## ğŸ³ Deployment en GCP

### Prerequisitos
- Cuenta de Google Cloud Platform
- `gcloud` CLI instalado
- Proyecto GCP creado

### Pasos de Deployment

```bash
# 1. Configurar proyecto
export GCP_PROJECT_ID="tu-proyecto-id"
gcloud config set project $GCP_PROJECT_ID

# 2. Build y push de imagen a GCR
docker build -t gcr.io/$GCP_PROJECT_ID/distributed-log:latest .
docker push gcr.io/$GCP_PROJECT_ID/distributed-log:latest

# 3. Crear VMs en 3 regiones diferentes
# RegiÃ³n 1: us-central1 (Iowa, USA)
gcloud compute instances create log-node-1 \
  --zone=us-central1-a \
  --machine-type=e2-micro \
  --image-family=debian-11 \
  --image-project=debian-cloud \
  --boot-disk-size=20GB \
  --tags=distributed-log,http-server \
  --metadata=NODE_ID=8001

# RegiÃ³n 2: europe-west1 (Belgium)
gcloud compute instances create log-node-2 \
  --zone=europe-west1-b \
  --machine-type=e2-micro \
  --image-family=debian-11 \
  --image-project=debian-cloud \
  --boot-disk-size=20GB \
  --tags=distributed-log,http-server \
  --metadata=NODE_ID=8002

# RegiÃ³n 3: asia-east1 (Taiwan)
gcloud compute instances create log-node-3 \
  --zone=asia-east1-a \
  --machine-type=e2-micro \
  --image-family=debian-11 \
  --image-project=debian-cloud \
  --boot-disk-size=20GB \
  --tags=distributed-log,http-server \
  --metadata=NODE_ID=8003

# 4. Configurar firewall
gcloud compute firewall-rules create allow-distributed-log \
  --allow=tcp:80,tcp:443,tcp:8000-8100 \
  --target-tags=distributed-log

# 5. SSH a cada VM y ejecutar el contenedor
# (Ver secciÃ³n "Deploy Manual en VM" mÃ¡s abajo)
```

### Deploy Manual en VM

```bash
# SSH a la VM
gcloud compute ssh log-node-1 --zone=us-central1-a

# Instalar Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Autenticar con GCR
gcloud auth configure-docker

# Pull imagen
docker pull gcr.io/$GCP_PROJECT_ID/distributed-log:latest

# Ejecutar contenedor
docker run -d \
  --name distributed-log \
  --restart unless-stopped \
  -p 80:80 \
  -e NODE_ID=8001 \
  gcr.io/$GCP_PROJECT_ID/distributed-log:latest

# Verificar
docker ps
docker logs distributed-log
```

## ğŸ“Š Estructura del Proyecto

```
.
â”œâ”€â”€ main.py                    # AplicaciÃ³n principal FastAPI
â”œâ”€â”€ server.py                  # Modelo de servidor
â”œâ”€â”€ lamport_clock.py          # ImplementaciÃ³n Reloj LÃ³gico (TODO)
â”œâ”€â”€ metrics.py                # Colector de mÃ©tricas (TODO)
â”œâ”€â”€ dockerfile                # Imagen Docker
â”œâ”€â”€ requirements.txt          # Dependencias Python
â”œâ”€â”€ test-local.sh            # Script para testing local
â”œâ”€â”€ stop-local.sh            # Script para detener cluster
â”œâ”€â”€ test-send-messages.sh    # Script para enviar mensajes de prueba
â””â”€â”€ README.md                # Este archivo
```

## ğŸ”§ Variables de Entorno

| Variable  | DescripciÃ³n                    | Ejemplo |
|-----------|--------------------------------|---------|
| NODE_ID   | ID Ãºnico del nodo (requerido)  | 8001    |

## ğŸ“ Logs

Los logs se muestran en stdout de cada contenedor:

```bash
# Ver logs en tiempo real
docker logs -f node1

# Ver Ãºltimas 100 lÃ­neas
docker logs --tail 100 node1

# Logs de todos los nodos
docker logs node1 > logs/node1.log
docker logs node2 > logs/node2.log
docker logs node3 > logs/node3.log
```

## ğŸ› Troubleshooting

### Los nodos no se comunican
```bash
# Verificar que estÃ¡n en la misma red Docker
docker network inspect distributed-net

# Verificar conectividad
docker exec node1 ping -c 3 node2
docker exec node1 ping -c 3 node3
```

### No se elige lÃ­der
```bash
# Verificar logs de elecciÃ³n
docker logs node1 | grep -i "election\|leader"

# Forzar re-elecciÃ³n matando el lÃ­der actual
docker stop node3
```

### Mensajes no se replican
```bash
# Verificar que el nodo es lÃ­der
curl http://localhost:8001/leader

# Verificar que followers estÃ¡n vivos
curl http://localhost:8002/health
curl http://localhost:8003/health
```

## ğŸ“š Referencias

- [Lamport Timestamps](https://lamport.azurewebsites.net/pubs/time-clocks.pdf) - Paper original de Leslie Lamport
- [Bully Algorithm](https://en.wikipedia.org/wiki/Bully_algorithm) - Wikipedia
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Docker Documentation](https://docs.docker.com/)

## ğŸ‘¥ Autores

- Sergio Sebastian Pezo Jimenez - RA: 298813
- Estudiante 2 - RA: XXXXXX

Projeto desenvolvido para a disciplina **MC714 - Sistemas DistribuÃ­dos**, Unicamp, 2Âº Semestre de 2025.

## ğŸ“„ Licencia

Este proyecto es para uso acadÃ©mico.
