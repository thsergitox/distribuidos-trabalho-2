# Arquitectura del Sistema Distribuido

## ğŸŒ DistribuciÃ³n GeogrÃ¡fica Global

### Regiones Seleccionadas (MÃ¡xima Distancia)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DISTRIBUCIÃ“N GLOBAL                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ‡ºğŸ‡¸ Node 1: Iowa, USA (us-central1-a)
   Latitud: 41.8780Â° N
   Longitud: 93.0977Â° W

ğŸ‡§ğŸ‡· Node 2: SÃ£o Paulo, Brasil (southamerica-east1-a)
   Latitud: 23.5505Â° S
   Longitud: 46.6333Â° W

ğŸ‡¦ğŸ‡º Node 3: Sydney, Australia (australia-southeast1-a)
   Latitud: 33.8688Â° S
   Longitud: 151.2093Â° E
```

### Distancias GeogrÃ¡ficas

| Desde â†’ Hasta | Distancia (km) | Latencia Estimada (ms) |
|---------------|----------------|------------------------|
| Iowa â†’ SÃ£o Paulo | ~9,500 km | 150-200 ms |
| Iowa â†’ Sydney | ~13,300 km | 200-250 ms |
| SÃ£o Paulo â†’ Sydney | ~13,600 km | 250-300 ms |

**Total de distancia recorrida:** >36,000 km (Â¡casi la circunferencia de la Tierra!)

### Â¿Por quÃ© estas regiones?

1. **MÃ¡xima separaciÃ³n geogrÃ¡fica:**
   - Cubrimos 3 continentes diferentes
   - Hemisferios norte y sur representados
   - MÃºltiples zonas horarias (diferencia de ~15 horas entre Iowa y Sydney)

2. **Simula un sistema distribuido REAL:**
   - Latencias altas (150-300ms) similares a aplicaciones globales reales
   - Diferentes condiciones de red
   - Prueba real del algoritmo Bully y Lamport bajo condiciones adversas

3. **Demuestra propiedades del sistema:**
   - **Reloj LÃ³gico de Lamport:** NO depende de sincronizaciÃ³n de relojes fÃ­sicos
   - **Algoritmo Bully:** Funciona incluso con latencias altas
   - **Tolerancia a fallos:** Si una regiÃ³n falla, las otras 2 continÃºan

## ğŸ—ï¸ Arquitectura de Deployment

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Google Cloud Platform                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  us-central1-a      â”‚  â”‚ southamerica-east1-aâ”‚  â”‚australia-southeast1-aâ”‚
â”‚  (Iowa, USA)        â”‚  â”‚ (SÃ£o Paulo, Brasil) â”‚  â”‚  (Sydney, Australia)â”‚
â”‚  IP: 34.55.87.209   â”‚  â”‚  IP: 34.95.212.100  â”‚  â”‚  IP: 35.201.29.184  â”‚
â”‚                     â”‚  â”‚                     â”‚  â”‚                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ VM: e2-micro â”‚   â”‚  â”‚  â”‚ VM: e2-micro â”‚   â”‚  â”‚  â”‚ VM: e2-micro â”‚   â”‚
â”‚  â”‚ 2 vCPU       â”‚   â”‚  â”‚  â”‚ 2 vCPU       â”‚   â”‚  â”‚  â”‚ 2 vCPU       â”‚   â”‚
â”‚  â”‚ 1GB RAM      â”‚   â”‚  â”‚  â”‚ 1GB RAM      â”‚   â”‚  â”‚  â”‚ 1GB RAM      â”‚   â”‚
â”‚  â”‚ 20GB Disk    â”‚   â”‚  â”‚  â”‚ 20GB Disk    â”‚   â”‚  â”‚  â”‚ 20GB Disk    â”‚   â”‚
â”‚  â”‚              â”‚   â”‚  â”‚  â”‚              â”‚   â”‚  â”‚  â”‚              â”‚   â”‚
â”‚  â”‚ Docker:      â”‚   â”‚  â”‚  â”‚ Docker:      â”‚   â”‚  â”‚  â”‚ Docker:      â”‚   â”‚
â”‚  â”‚ - FastAPI    â”‚   â”‚  â”‚  â”‚ - FastAPI    â”‚   â”‚  â”‚  â”‚ - FastAPI    â”‚   â”‚
â”‚  â”‚ - Lamport    â”‚   â”‚  â”‚  â”‚ - Lamport    â”‚   â”‚  â”‚  â”‚ - Lamport    â”‚   â”‚
â”‚  â”‚ - Bully      â”‚   â”‚  â”‚  â”‚ - Bully      â”‚   â”‚  â”‚  â”‚ - Bully      â”‚   â”‚
â”‚  â”‚              â”‚   â”‚  â”‚  â”‚              â”‚   â”‚  â”‚  â”‚              â”‚   â”‚
â”‚  â”‚ NODE_ID:8001 â”‚   â”‚  â”‚  â”‚ NODE_ID:8002 â”‚   â”‚  â”‚  â”‚ NODE_ID:8003 â”‚   â”‚
â”‚  â”‚ Port: 80     â”‚   â”‚  â”‚  â”‚ Port: 80     â”‚   â”‚  â”‚  â”‚ Port: 80     â”‚   â”‚
â”‚  â”‚              â”‚   â”‚  â”‚  â”‚              â”‚   â”‚  â”‚  â”‚              â”‚   â”‚
â”‚  â”‚ OTHER_SERVERSâ”‚   â”‚  â”‚  â”‚ OTHER_SERVERSâ”‚   â”‚  â”‚  â”‚ OTHER_SERVERSâ”‚   â”‚
â”‚  â”‚ = "IPs:..."  â”‚   â”‚  â”‚  â”‚ = "IPs:..."  â”‚   â”‚  â”‚  â”‚ = "IPs:..."  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        ComunicaciÃ³n HTTP/REST usando IPs pÃºblicas
        (Internet - Latencias reales de 150-300ms)
```

**Nota Importante sobre Networking:**
- En **Docker local**: Los nodos usan nombres de contenedor (node1, node2, node3)
- En **GCP**: Los nodos usan IPs pÃºblicas pasadas via variable `OTHER_SERVERS`
- El cÃ³digo detecta automÃ¡ticamente el entorno y se configura apropiadamente

## ğŸ”„ Flujo de ComunicaciÃ³n

### 1. ElecciÃ³n de LÃ­der (Algoritmo Bully)

```
Inicio: Todos los nodos inician simultÃ¡neamente

Node 8001 (Iowa):     "Â¿Hay alguien con ID mayor?"
                      â†’ Consulta a 8002 (Brasil) [~180ms RTT]
                      â†’ Consulta a 8003 (Sydney) [~230ms RTT]

Node 8002 (Brasil):   "Â¿Hay alguien con ID mayor?"
                      â†’ Consulta a 8003 (Sydney) [~270ms RTT]

Node 8003 (Sydney):   "No hay nadie mayor, soy el lÃ­der"
                      â†’ Notifica a todos [~250ms promedio]

Resultado: Node 8003 es el LÃDER
Tiempo total de elecciÃ³n: ~1-2 segundos
```

### 2. ReplicaciÃ³n de Mensajes (Reloj LÃ³gico de Lamport)

```
Cliente â†’ Node 8001 (Iowa):
  POST /?message=Hello

Node 8001:
  1. Detecta que NO es lÃ­der
  2. Forward a Node 8003 (Sydney) [~230ms]

Node 8003 (LÃ­der):
  1. Incrementa Lamport Clock: t=1
  2. Crea mensaje: {id: 2, lamport: 1, node: 8003}
  3. Replica en PARALELO:
     â†’ Node 8001 (Iowa)   [~230ms]
     â†’ Node 8002 (Brasil) [~270ms]

Node 8001 y 8002:
  1. Reciben mensaje con lamport=1
  2. Actualizan reloj: max(local, 1) + 1
  3. Guardan mensaje ordenado por Lamport
  4. Responden al lÃ­der

Tiempo total: ~500-600ms (incluyendo latencias globales)
```

## ğŸ“Š MÃ©tricas Observables

### Latencias Esperadas

| OperaciÃ³n | Latencia Estimada |
|-----------|-------------------|
| Lectura local (GET /messages) | 1-5 ms |
| Escritura en lÃ­der (POST /) | 10-20 ms |
| ReplicaciÃ³n global completa | 300-600 ms |
| ElecciÃ³n de lÃ­der (re-election) | 1-2 segundos |
| Health check entre nodos | 150-300 ms |

### Propiedades Garantizadas

âœ… **Consistencia Causal (Lamport):**
   - Si mensaje A â†’ B (causalmente), entonces Lamport(A) < Lamport(B)
   - SIEMPRE, independientemente de latencias de red

âœ… **Disponibilidad (Bully):**
   - Si 2 de 3 nodos estÃ¡n vivos, el sistema funciona
   - Re-elecciÃ³n automÃ¡tica en ~1-2 segundos

âœ… **Tolerancia a Particiones:**
   - Cada nodo puede seguir operando localmente
   - Eventual consistency cuando la red se recupera

## ğŸ”Œ APIs Requeridas en GCP

Para que el deployment funcione correctamente, necesitas habilitar estas APIs:

```bash
# Compute Engine API - Para crear y gestionar VMs
gcloud services enable compute.googleapis.com

# Artifact Registry API - Para almacenar imÃ¡genes Docker (nuevo sistema)
gcloud services enable artifactregistry.googleapis.com

# Container Registry API - Para backward compatibility con gcr.io
gcloud services enable containerregistry.googleapis.com
```

**Nota:** Aunque usamos `gcr.io` en el cÃ³digo, Google Cloud internamente redirige a Artifact Registry, por lo que ambas APIs son necesarias.

## ğŸ’° Costos Estimados (GCP)

```
VM e2-micro (3 instancias):
  - Precio: ~$6.11/mes por instancia
  - Total VMs: ~$18.33/mes

Egress Traffic (datos saliendo de GCP):
  - Primeros 1GB/mes: Gratis
  - Siguiente 10TB: $0.12/GB
  - Estimado para testing: ~$5/mes

TOTAL ESTIMADO: ~$25/mes

Para este proyecto (algunas horas): < $1
```

## ğŸ”’ Seguridad

### Firewall Rules

```
allow-distributed-log:
  - Protocolo: TCP
  - Puertos: 80, 443, 8000-8100
  - Fuente: 0.0.0.0/0 (cualquier IP)
  - Target: VMs con tag "distributed-log"

allow-ssh-distributed-log:
  - Protocolo: TCP
  - Puerto: 22
  - Fuente: 0.0.0.0/0
  - Target: VMs con tag "distributed-log"
```

### Mejoras de Seguridad (ProducciÃ³n)

âš ï¸ Para un sistema de producciÃ³n, implementar:
- HTTPS con certificados TLS
- AutenticaciÃ³n entre nodos (tokens JWT)
- IP whitelisting (solo IPs de nodos conocidos)
- VPN o VPC peering privado
- Rate limiting
- DDoS protection (Cloud Armor)
