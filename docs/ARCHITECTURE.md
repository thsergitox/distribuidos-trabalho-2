# Arquitetura do Sistema DistribuÃ­do

## ğŸŒ DistribuiÃ§Ã£o GeogrÃ¡fica Global

### RegiÃµes Selecionadas (MÃ¡xima DistÃ¢ncia)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DISTRIBUIÃ‡ÃƒO GLOBAL                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ‡ºğŸ‡¸ Node 1: Iowa, USA (us-central1-a)
   Latitude: 41.8780Â° N
   Longitude: 93.0977Â° W

ğŸ‡§ğŸ‡· Node 2: SÃ£o Paulo, Brasil (southamerica-east1-a)
   Latitude: 23.5505Â° S
   Longitude: 46.6333Â° W

ğŸ‡¦ğŸ‡º Node 3: Sydney, AustrÃ¡lia (australia-southeast1-a)
   Latitude: 33.8688Â° S
   Longitude: 151.2093Â° E
```

### DistÃ¢ncias GeogrÃ¡ficas

| De â†’ Para | DistÃ¢ncia (km) | LatÃªncia Estimada (ms) |
|-----------|----------------|------------------------|
| Iowa â†’ SÃ£o Paulo | ~9,500 km | 150-200 ms |
| Iowa â†’ Sydney | ~13,300 km | 200-250 ms |
| SÃ£o Paulo â†’ Sydney | ~13,600 km | 250-300 ms |

**DistÃ¢ncia total percorrida:** >36.000 km (quase a circunferÃªncia da Terra!)

### Por que estas regiÃµes?

1. **MÃ¡xima separaÃ§Ã£o geogrÃ¡fica:**
   - Cobrimos 3 continentes diferentes
   - HemisfÃ©rios norte e sul representados
   - MÃºltiplos fusos horÃ¡rios (diferenÃ§a de ~15 horas entre Iowa e Sydney)

2. **Simula um sistema distribuÃ­do REAL:**
   - LatÃªncias altas (150-300ms) similares a aplicaÃ§Ãµes globais reais
   - Diferentes condiÃ§Ãµes de rede
   - Teste real do algoritmo Bully e Lamport sob condiÃ§Ãµes adversas

3. **Demonstra propriedades do sistema:**
   - **RelÃ³gio LÃ³gico de Lamport:** NÃƒO depende de sincronizaÃ§Ã£o de relÃ³gios fÃ­sicos
   - **Algoritmo Bully:** Funciona mesmo com latÃªncias altas
   - **TolerÃ¢ncia a falhas:** Se uma regiÃ£o falha, as outras 2 continuam

## ğŸ—ï¸ Arquitetura de Deployment

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Google Cloud Platform                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  us-central1-a      â”‚  â”‚ southamerica-east1-aâ”‚  â”‚australia-southeast1-aâ”‚
â”‚  (Iowa, USA)        â”‚  â”‚ (SÃ£o Paulo, Brasil) â”‚  â”‚  (Sydney, AustrÃ¡lia)â”‚
â”‚  IP: 34.55.87.209   â”‚  â”‚  IP: 34.95.212.100  â”‚  â”‚  IP: 35.201.29.184  â”‚
â”‚                     â”‚  â”‚                     â”‚  â”‚                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ VM: e2-micro â”‚   â”‚  â”‚  â”‚ VM: e2-micro â”‚   â”‚  â”‚  â”‚ VM: e2-micro â”‚   â”‚
â”‚  â”‚ 2 vCPU       â”‚   â”‚  â”‚  â”‚ 2 vCPU       â”‚   â”‚  â”‚  â”‚ 2 vCPU       â”‚   â”‚
â”‚  â”‚ 1GB RAM      â”‚   â”‚  â”‚  â”‚ 1GB RAM      â”‚   â”‚  â”‚  â”‚ 1GB RAM      â”‚   â”‚
â”‚  â”‚ 20GB Disk    â”‚   â”‚  â”‚  â”‚ 20GB Disk    â”‚   â”‚  â”‚  â”‚ 20GB Disk    â”‚   â”‚
â”‚  â”‚              â”‚   â”‚  â”‚  â”‚              â”‚   â”‚  â”‚  â”‚              â”‚   â”‚
â”‚  â”‚ Docker:      â”‚   â”‚  â”‚  â”‚ Docker:      â”‚   â”‚  â”‚  â”‚ Docker:      â”‚   â”‚
â”‚  â”‚ - FastAPI    â”‚   â”‚  â”‚  â”‚ - FastAPI    â”‚   â”‚  â”‚  â”‚ - FastAPI    â”‚   â”‚
â”‚  â”‚ - Lamport    â”‚   â”‚  â”‚  â”‚ - Lamport    â”‚   â”‚  â”‚  â”‚ - Lamport    â”‚   â”‚
â”‚  â”‚ - Bully      â”‚   â”‚  â”‚  â”‚ - Bully      â”‚   â”‚  â”‚  â”‚ - Bully      â”‚   â”‚
â”‚  â”‚              â”‚   â”‚  â”‚  â”‚              â”‚   â”‚  â”‚  â”‚              â”‚   â”‚
â”‚  â”‚ NODE_ID:8001 â”‚   â”‚  â”‚  â”‚ NODE_ID:8002 â”‚   â”‚  â”‚  â”‚ NODE_ID:8003 â”‚   â”‚
â”‚  â”‚ Port: 80     â”‚   â”‚  â”‚  â”‚ Port: 80     â”‚   â”‚  â”‚  â”‚ Port: 80     â”‚   â”‚
â”‚  â”‚              â”‚   â”‚  â”‚  â”‚              â”‚   â”‚  â”‚  â”‚              â”‚   â”‚
â”‚  â”‚ OTHER_SERVERSâ”‚   â”‚  â”‚  â”‚ OTHER_SERVERSâ”‚   â”‚  â”‚  â”‚ OTHER_SERVERSâ”‚   â”‚
â”‚  â”‚ = "IPs:..."  â”‚   â”‚  â”‚  â”‚ = "IPs:..."  â”‚   â”‚  â”‚  â”‚ = "IPs:..."  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        ComunicaÃ§Ã£o HTTP/REST usando IPs pÃºblicos
        (Internet - LatÃªncias reais de 150-300ms)
```

**Nota Importante sobre Networking:**
- No **Docker local**: Os nodos usam nomes de container (node1, node2, node3)
- No **GCP**: Os nodos usam IPs pÃºblicos passados via variÃ¡vel `OTHER_SERVERS`
- O cÃ³digo detecta automaticamente o ambiente e se configura apropriadamente

## ğŸ”„ Fluxo de ComunicaÃ§Ã£o

### 1. EleiÃ§Ã£o de LÃ­der (Algoritmo Bully)

```
InÃ­cio: Todos os nodos iniciam simultaneamente

Node 8001 (Iowa):     "HÃ¡ alguÃ©m com ID maior?"
                      â†’ Consulta a 8002 (Brasil) [~180ms RTT]
                      â†’ Consulta a 8003 (Sydney) [~230ms RTT]

Node 8002 (Brasil):   "HÃ¡ alguÃ©m com ID maior?"
                      â†’ Consulta a 8003 (Sydney) [~270ms RTT]

Node 8003 (Sydney):   "NÃ£o hÃ¡ ninguÃ©m maior, sou o lÃ­der"
                      â†’ Notifica a todos [~250ms promedio]

Resultado: Node 8003 Ã© o LÃDER
Tempo total de eleiÃ§Ã£o: ~1-2 segundos
```

### 2. ReplicaÃ§Ã£o de Mensagens (RelÃ³gio LÃ³gico de Lamport)

```
Cliente â†’ Node 8001 (Iowa):
  POST /?message=Hello

Node 8001:
  1. Detecta que NÃƒO Ã© lÃ­der
  2. Forward a Node 8003 (Sydney) [~230ms]

Node 8003 (LÃ­der):
  1. Incrementa Lamport Clock: t=1
  2. Cria mensagem: {id: 2, lamport: 1, node: 8003}
  3. Replica em PARALELO:
     â†’ Node 8001 (Iowa)   [~230ms]
     â†’ Node 8002 (Brasil) [~270ms]

Node 8001 e 8002:
  1. Recebem mensagem com lamport=1
  2. Atualizam relÃ³gio: max(local, 1) + 1
  3. Guardam mensagem ordenada por Lamport
  4. Respondem ao lÃ­der

Tempo total: ~500-600ms (incluindo latÃªncias globais)
```

## ğŸ“Š MÃ©tricas ObservÃ¡veis

### LatÃªncias Esperadas

| OperaÃ§Ã£o | LatÃªncia Estimada |
|----------|-------------------|
| Leitura local (GET /messages) | 1-5 ms |
| Escrita no lÃ­der (POST /) | 10-20 ms |
| ReplicaÃ§Ã£o global completa | 300-600 ms |
| EleiÃ§Ã£o de lÃ­der (re-election) | 1-2 segundos |
| Health check entre nodos | 150-300 ms |

### Propriedades Garantidas

âœ… **ConsistÃªncia Causal (Lamport):**
   - Se mensagem A â†’ B (causalmente), entÃ£o Lamport(A) < Lamport(B)
   - SEMPRE, independentemente de latÃªncias de rede

âœ… **Disponibilidade (Bully):**
   - Se 2 de 3 nodos estÃ£o vivos, o sistema funciona
   - Re-eleiÃ§Ã£o automÃ¡tica em ~1-2 segundos

âœ… **TolerÃ¢ncia a PartiÃ§Ãµes:**
   - Cada nodo pode seguir operando localmente
   - ConsistÃªncia eventual quando a rede se recupera

## ğŸ”Œ APIs Requeridas no GCP

Para que o deployment funcione corretamente, vocÃª precisa habilitar estas APIs:

```bash
# Compute Engine API - Para criar e gerenciar VMs
gcloud services enable compute.googleapis.com

# Artifact Registry API - Para armazenar imagens Docker (novo sistema)
gcloud services enable artifactregistry.googleapis.com

# Container Registry API - Para backward compatibility com gcr.io
gcloud services enable containerregistry.googleapis.com
```

**Nota:** Embora usemos `gcr.io` no cÃ³digo, Google Cloud internamente redireciona para Artifact Registry, portanto ambas as APIs sÃ£o necessÃ¡rias.

## ğŸ’° Custos Estimados (GCP)

```
VM e2-micro (3 instÃ¢ncias):
  - PreÃ§o: ~$6.11/mÃªs por instÃ¢ncia
  - Total VMs: ~$18.33/mÃªs

Egress Traffic (dados saindo do GCP):
  - Primeiros 1GB/mÃªs: GrÃ¡tis
  - PrÃ³ximos 10TB: $0.12/GB
  - Estimado para testing: ~$5/mÃªs

TOTAL ESTIMADO: ~$25/mÃªs

Para este projeto (algumas horas): < $1
```

## ğŸ”’ SeguranÃ§a

### Firewall Rules

```
allow-distributed-log:
  - Protocolo: TCP
  - Portas: 80, 443, 8000-8100
  - Fonte: 0.0.0.0/0 (qualquer IP)
  - Target: VMs com tag "distributed-log"

allow-ssh-distributed-log:
  - Protocolo: TCP
  - Porta: 22
  - Fonte: 0.0.0.0/0
  - Target: VMs com tag "distributed-log"
```

### Melhorias de SeguranÃ§a (ProduÃ§Ã£o)

âš ï¸ Para um sistema de produÃ§Ã£o, implementar:
- HTTPS com certificados TLS
- AutenticaÃ§Ã£o entre nodos (tokens JWT)
- IP whitelisting (somente IPs de nodos conhecidos)
- VPN ou VPC peering privado
- Rate limiting
- DDoS protection (Cloud Armor)
